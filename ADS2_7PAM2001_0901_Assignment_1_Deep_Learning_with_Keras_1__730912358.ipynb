{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BranMwangi/BranMwangi/blob/main/ADS2_7PAM2001_0901_Assignment_1_Deep_Learning_with_Keras_1__730912358.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cuL02iE742P"
      },
      "source": [
        "# Assignment 1 - Deep Learning with Keras\n",
        "\n",
        "This assignment will test you on the following skills: implementing a convolutional neural network in Keras, utilising the Functional Model API, building custom layers and loss functions, and analysing a trained model.\n",
        "\n",
        "The model you will be building is called a Variational Autoencoder (VAE), a special type of neural network that uses Bayesian Inference to generate synthetic data. As described in the lectures, a VAE compresses a sample input into a low dimensional space (a latent vector). A constraint is applied in the form of a modification to the loss function, which has the effect of forcing the latent vector to look like a standard normal distribution.\n",
        "\n",
        "This notebook is divided into sections, which you should use to complete the following tasks:\n",
        "\n",
        "1. Implement a custom layer that performs the \"reparameterization trick\" described in the lectures.\n",
        "2. Use the functional model API in keras to create an encoder model and a decoder model, using the specifications provided. Combine these models into the full VAE architecture.\n",
        "3. Train the model using the celeb_a dataset of celebrity faces.\n",
        "4. Create plots that demonstrate the ability of the network to reconstruct images, and generate new images.\n",
        "\n",
        "In addition to submitting your completed notebook, you should write a 1-page report that discusses the results of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jedCEAhxvx9S"
      },
      "outputs": [],
      "source": [
        "# Module imports\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8VVxISTzeTo"
      },
      "source": [
        "# Task 1 - Reparameterisation layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pwRt4ziVfckg"
      },
      "outputs": [],
      "source": [
        "### Create a class called latent_sampling, which subclasses layers.Layer.\n",
        "### The class should perform the reparameterisation trick in its .call()\n",
        "### method.\n",
        "### Reparameterization Trick: z = mean + epsilon * exp(ln(variance) * 0.5)\n",
        "### epsilon = N(0,1), a unit normal with same dims as mean and variance\n",
        "\n",
        "# Include the follow two lines in your .call method:\n",
        "# self.add_loss(-0.5 * tf.reduce_sum(1 + logvar - tf.square(mean) - tf.exp(logvar)))\n",
        "# self.add_metric(-0.5 * tf.reduce_sum(1 + logvar - tf.square(mean) - tf.exp(logvar)), name='kl_loss')\n",
        "\n",
        "class latent_sampling(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(latent_sampling, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, mean, logvar):\n",
        "        # Sample noise from a standard normal distribution\n",
        "        epsilon = tf.random.normal(shape=tf.shape(mean))\n",
        "        # Perform the reparameterization trick\n",
        "        z = mean + epsilon * tf.exp(logvar * 0.5)\n",
        "        # Add the KL divergence loss to the model\n",
        "        self.add_loss(-0.5 * tf.reduce_sum(1 + logvar - tf.square(mean) - tf.exp(logvar)))\n",
        "        self.add_metric(-0.5 * tf.reduce_sum(1 + logvar - tf.square(mean) - tf.exp(logvar)), name='kl_loss')\n",
        "        return z\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uvplEws4ILf"
      },
      "source": [
        "# Task 2 - Model Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nLmbT04yrcIm"
      },
      "outputs": [],
      "source": [
        "### Create the encoder model, using the functional API and the architecture\n",
        "### detailed below. Use tf.keras.models.Model to initialise the model.\n",
        "\n",
        "# Model: \"encoder\"\n",
        "# ____________________________________________________________________________________________________\n",
        "#  Layer (type)            Output Shape           Activation  kernel_size  padding  Input\n",
        "# ====================================================================================================\n",
        "#  enc_input (InputLayer)  [(None, 128, 128, 3)]  None\n",
        "#  enc_conv_1 (Conv2D)     (None, 64, 64, 32)     ReLU        (3,3)        'same'   enc_input\n",
        "#  enc_conv_2 (Conv2D)     (None, 32, 32, 64)     ReLU        (3,3)        'same'   enc_conv_1\n",
        "#  enc_conv_3 (Conv2D)     (None, 16, 16, 64)     ReLU        (3,3)        'same'   enc_conv_2    \n",
        "#  enc_conv_4 (Conv2D)     (None, 8, 8, 64)       ReLU        (3,3)        'same'   enc_conv_3    \n",
        "#  enc_flat (Flatten)      (None, 4096)           None        None         None     enc_conv_4\n",
        "#  z_mean (Dense)          (None, 200)            None        None         None     enc_flat                        \n",
        "#  z_log_var (Dense)       (None, 200)            None        None         None     enc_flat\n",
        "#  z (latent_sampling)     (None, 200)            None        None         None     (z_mean, z_log_var)\n",
        "\n",
        "# Define the input layer\n",
        "enc_input = tf.keras.layers.Input(shape=(128, 128, 3))\n",
        "\n",
        "# Add a sequence of convolutional layers\n",
        "x = tf.keras.layers.Conv2D(32, 3, activation='relu', padding='same')(enc_input)\n",
        "x = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
        "x = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
        "x = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
        "\n",
        "# Add a flatten layer to convert the 2D convolutional feature maps into a 1D feature vector\n",
        "x = tf.keras.layers.Flatten()(x)\n",
        "\n",
        "# Add a dense layer to map the feature vector to the latent space\n",
        "z_mean = tf.keras.layers.Dense(200)(x)\n",
        "z_log_var = tf.keras.layers.Dense(200)(x)\n",
        "\n",
        "# Add the latent_sampling layer\n",
        "z = latent_sampling()(z_mean, z_log_var)\n",
        "\n",
        "# Create the encoder model\n",
        "encoder = tf.keras.Model(inputs=enc_input, outputs=z)\n",
        "\n",
        "# Encode an input tensor\n",
        "inputs = tf.random.normal((32, 128, 128, 3))\n",
        "latent = encoder(inputs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QJWR0m2HjFzF"
      },
      "outputs": [],
      "source": [
        "### Create the decoder model, using the functional API and the architecture\n",
        "### detailed below. Use tf.keras.models.Model to initialise the model.\n",
        "\n",
        "# Model: \"decoder\"\n",
        "# ____________________________________________________________________________________________________\n",
        "#  Layer (type)                   Output Shape           Activation  kernel_size  padding  Input\n",
        "# ====================================================================================================\n",
        "#  dec_input (InputLayer)         [(None, 200)]          None\n",
        "#  dec_dense (Dense)              (None, 4096)           ReLU        None         None     dec_input\n",
        "#  dec_reshape (Reshape)          (None, 8, 8, 64)       None        None         None     dec_dense\n",
        "#  dec_conv_1 (Conv2DTranspose)   (None, 8, 8, 64)       ReLU        (3,3)        'same'   dec_reshape\n",
        "#  dec_conv_2 (Conv2DTranspose)   (None, 16, 16, 64)     ReLU        (3,3)        'same'   dec_conv_1\n",
        "#  dec_conv_3 (Conv2DTranspose)   (None, 32, 32, 64)     ReLU        (3,3)        'same'   dec_conv_2    \n",
        "#  dec_conv_4 (Conv2DTranspose)   (None, 64, 64, 32)     ReLU        (3,3)        'same'   dec_conv_3    \n",
        "#  dec_output (Conv2DTranspose)   (None, 128, 128, 3)    ReLU        (3,3)        'same'   dec_conv_4\n",
        "\n",
        "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape\n",
        "\n",
        "#dec_input = Input(shape=(200,))\n",
        "#dec_dense = Dense(units=4096, activation='relu')(dec_input)\n",
        "#dec_reshape = Reshape(target_shape=(8, 8, 64))(dec_dense)\n",
        "#dec_conv_1 = Conv2DTranspose(filters=64, kernel_size=(3, 3), padding='same', activation='relu')(dec_reshape)\n",
        "#dec_conv_2 = Conv2DTranspose(filters=64, kernel_size=(3, 3), padding='same', activation='relu')(dec_conv_1)\n",
        "#dec_conv_3 = Conv2DTranspose(filters=64, kernel_size=(3, 3), padding='same', activation='relu')(dec_conv_2)\n",
        "#dec_conv_4 = Conv2DTranspose(filters=32, kernel_size=(3, 3), padding='same', activation='relu')(dec_conv_3)\n",
        "#dec_output = Conv2DTranspose(filters=3, kernel_size=(3, 3), padding='same', activation='relu')(dec_conv_4)\n",
        "#decoder_model = Model(inputs=dec_input, outputs=dec_output)\n",
        "\n",
        "# Define the input layer for the decoder\n",
        "dec_input = tf.keras.layers.Input(shape=(200,))\n",
        "\n",
        "# Add a sequence of dense and reshape layers to the decoder\n",
        "x = tf.keras.layers.Dense(4096, activation='relu')(dec_input)\n",
        "x = tf.keras.layers.Reshape((8, 8, 64))(x)\n",
        "\n",
        "# Add a sequence of convolutional layers to the decoder\n",
        "x = tf.keras.layers.Conv2DTranspose(64, 3, activation='relu', padding='same')(x)\n",
        "x = tf.keras.layers.Conv2DTranspose(64, 3, activation='relu', padding='same')(x)\n",
        "x = tf.keras.layers.Conv2DTranspose(64, 3, activation='relu', padding='same')(x)\n",
        "x = tf.keras.layers.Conv2DTranspose(32, 3, activation='relu', padding='same')(x)\n",
        "\n",
        "# Add the output layer for the decoder\n",
        "dec_output = tf.keras.layers.Conv2DTranspose(3, 3, activation='relu', padding='same')(x)\n",
        "\n",
        "# Create the decoder model\n",
        "decoder = tf.keras.Model(inputs=dec_input, outputs=dec_output)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wQgVcTLJQHSr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "f9b1e715-466c-4534-b36f-8b2a600d7b4b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4157be9ab533>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Map the feature vector to the latent space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mz_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mz_log_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/backend.py\u001b[0m in \u001b[0;36mrandom_uniform\u001b[0;34m(self, shape, minval, maxval, dtype)\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._generator.uniform(\n\u001b[1;32m   1919\u001b[0m           shape=shape, minval=minval, maxval=maxval, dtype=dtype)\n\u001b[0;32m-> 1920\u001b[0;31m     return tf.random.uniform(\n\u001b[0m\u001b[1;32m   1921\u001b[0m         \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m         seed=self.make_legacy_seed())\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: failed to allocate memory [Op:AddV2]"
          ]
        }
      ],
      "source": [
        "### Create the VAE model, again using tf.keras.models.Model, with the function\n",
        "### API to combine the feed the outputs of the encoder into the inputs of the\n",
        "### decoder.\n",
        "\n",
        "#Import the necessary layers from TensorFlow:\n",
        "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape\n",
        "\n",
        "#Define the input layer for the encoder model by using the Input function and specify the shape of the input tensor:\n",
        "enc_input = Input(shape=(128, 128, 3))\n",
        "\n",
        "# Modify the number of filters in the Conv2D layers in the encoder so that the final feature map has a shape of [batch_size, 2, 2, 1600]\n",
        "x = Conv2D(32, 3, activation='relu', padding='same')(enc_input)\n",
        "x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
        "x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
        "x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
        "x = Conv2D(1600, 3, activation='relu', padding='same')(x)\n",
        "\n",
        "# Flatten the final feature map to convert it into a 1D feature vector\n",
        "x = Flatten()(x)\n",
        "\n",
        "# Map the feature vector to the latent space\n",
        "z_mean = Dense(50)(x)\n",
        "z_log_var = Dense(50)(x)\n",
        "\n",
        "# Sample from the latent space\n",
        "z = latent_sampling()(z_mean, z_log_var)\n",
        "\n",
        "# Initialize the decoder model\n",
        "dec_input = Input(shape=(50,))\n",
        "dec_dense = Dense(units=1600, activation='relu')(dec_input)\n",
        "\n",
        "# Reshape the decoder output to the desired shape [batch_size, 2, 2, 1600]\n",
        "dec_reshape = x = Reshape(target_shape=(2, 2, 1600))(x)\n",
        "\n",
        "# Add the transpose convolutional layers to the decoder model\n",
        "x = Conv2DTranspose(filters=64, kernel_size=(3, 3), padding='same', activation='relu')(x)\n",
        "x = Conv2DTranspose(filters=64, kernel_size=(3, 3), padding='same', activation='relu')(x)\n",
        "x = Conv2DTranspose(filters=64, kernel_size=(3, 3), padding='same', activation='relu')(x)\n",
        "x = Conv2DTranspose(filters=64, kernel_size=(3, 3), padding='same', activation='relu')(x)\n",
        "x = Conv2DTranspose(filters=32, kernel_size=(3, 3), padding='same', activation='relu')(x)\n",
        "\n",
        "# Add the output layer to the decoder model\n",
        "outputs = Conv2DTranspose(filters=3, kernel_size=(3, 3), padding='same', activation='relu')(x)\n",
        "\n",
        "# Initialize the encoder and decoder models\n",
        "encoder = Model(inputs=enc_input, outputs=z)\n",
        "decoder = Model(inputs=dec_input, outputs=outputs)\n",
        "\n",
        "# Use the encoder model to map the input tensor to the latent space\n",
        "latent = encoder(inputs)\n",
        "\n",
        "# Use the decoder model to map the latent tensor back to the original space\n",
        "outputs = decoder(latent)\n",
        "\n",
        "# Initialize the VAE model\n",
        "vae = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SjEJH_1QsOZ"
      },
      "source": [
        "# Task 3 - Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMtLu5S9hdi2"
      },
      "outputs": [],
      "source": [
        "# Provided here are the loss functions for the VAE model.\n",
        "\n",
        "def recon_loss(y_true, y_pred):\n",
        "    recon = tf.reduce_sum(tf.square(y_true-y_pred), axis=(1,2,3))\n",
        "    return tf.reduce_mean(recon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2M0QU9BNmGx1"
      },
      "outputs": [],
      "source": [
        "# This code is provided to load a subsample of the celeb_a dataset, and process\n",
        "# the images into the correct format for the model.\n",
        "\n",
        "\n",
        "def img_process(features):\n",
        "    \"\"\"\n",
        "    A preprocessing fuction for the test and validation datasets. This function\n",
        "    accepts the oxford_iiit_pet dataset, extracts the images and species label,\n",
        "    and resizes and rescales the images.\n",
        "    \"\"\"\n",
        "    image = tf.image.resize(features['image'], (128,128))\n",
        "    image = tf.cast(image, 'float32')/255.\n",
        "    return image, image\n",
        "\n",
        "\n",
        "train_ds, test_ds = tfds.load('celeb_a', split=['train[:10%]', 'test[:10%]'], download=True, shuffle_files=True)\n",
        "train_ds = train_ds.map(img_process).cache().batch(64)\n",
        "test_ds = test_ds.map(img_process).cache().batch(64)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaJG9ZNKtHFh"
      },
      "outputs": [],
      "source": [
        "### Compile the VAE model, choosing an appropriate optimizer and learning rate,\n",
        "### the total_loss function as the model loss, and any appropriate metrics.\n",
        "\n",
        "#optimizer: This argument specifies the optimization algorithm to use for training the model. You can choose an appropriate optimizer such as Adam, SGD, or RMSprop, depending on your specific model and task. For example, you can use the Adam optimizer as follows:\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "#loss: This argument specifies the loss function to use for training the model. In the case of a VAE model, you can use the total loss function, which is the sum of the reconstruction loss and the KL divergence loss. For example:\n",
        "loss = total_loss\n",
        "\n",
        "#metrics: This argument specifies the metrics to use for evaluating the model. You can choose any appropriate metrics such as accuracy, precision, or AUC, depending on your specific model and task. For example:\n",
        "metrics = ['accuracy']\n",
        "\n",
        "#With these arguments, you can compile the VAE model as follows\n",
        "vae.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "#This will compile the VAE model, using the specified optimizer, loss function, and metrics. You can then use the compiled model to fit the training data, evaluate the model on the test data, and make predictions on new data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLCBFs52tVHy"
      },
      "outputs": [],
      "source": [
        "### Train the model using the train dataset for an appropriate number of epochs.\n",
        "### Store the losses and metrics in the history dictionary.\n",
        "\n",
        "#You can store the losses and metrics in the history dictionary by setting the history argument of the fit method to a dictionary object\n",
        "history = {'loss': [], 'accuracy': []}\n",
        "\n",
        "#With these arguments, you can train the VAE model as follows:\n",
        "history = model.fit(x=train_ds, y=train_ds, batch_size=64, epochs=10, \n",
        "validation_data=(test_ds, test_ds), callbacks=None, history=history)\n",
        "\n",
        "\n",
        "\n",
        "#This will train the VAE model on the training dataset for 10 epochs, using a batch size of 64 and storing the losses and metrics in the history dictionary. You can then access the history of the training process by using the history attribute of the Model object. For example, you can access the loss values for each epoch as follows:\n",
        "loss_values = history.history['loss']\n",
        "\n",
        "#You can also access the metric values for each epoch as follows:\n",
        "accuracy_values = history.history['accuracy']\n",
        "\n",
        "#This will allow you to plot the training and validation losses and metrics over time, and monitor the progress of the model during training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXKzKJpyvrQ6"
      },
      "source": [
        "# Task 4 - Analyse the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zU5SfXftn2U"
      },
      "outputs": [],
      "source": [
        "### Plot the losses and metrics. Comment on the figures in your report, with\n",
        "### regard to how the training has proceeded.\n",
        "\n",
        "#To plot the losses and metrics of the VAE model, you can use the matplotlib library in Python. Here is an example of how you can do this:\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot the training and validation losses\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training Loss', 'Validation Loss'], loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "# Plot the training and validation metrics\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training Accuracy', 'Validation Accuracy'], loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JU_LFgrnt4EU"
      },
      "outputs": [],
      "source": [
        "### Using the test dataset, create a plot that shows the reconstruction quality\n",
        "### of the training model. Comment on the results in your report.\n",
        "\n",
        "# Generate reconstructions of the test images\n",
        "reconstructions = vae.predict(test_ds)\n",
        "\n",
        "# Select a random subset of the test images and reconstructions\n",
        "num_samples = 10\n",
        "indices = np.random.randint(0, len(test_ds), num_samples)\n",
        "test_images = test_ds[indices]\n",
        "recon_images = reconstructions[indices]\n",
        "\n",
        "# Plot the test images and reconstructions\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(num_samples):\n",
        "    plt.subplot(num_samples, 2, 2*i+1)\n",
        "    plt.imshow(test_images[i])\n",
        "    plt.axis('off')\n",
        "    plt.title('Original')\n",
        "    plt.subplot(num_samples, 2, 2*i+2)\n",
        "    plt.imshow(recon_images[i])\n",
        "    plt.axis('\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ursPAnp2uBEv"
      },
      "outputs": [],
      "source": [
        "### Demonstrate the generative properties of the VAE by drawing randomly sampled\n",
        "### latent vectors from a unit Guassian and passing them to the train decoder.\n",
        "### Plot the results and comment on them in your report.\n",
        "\n",
        "# Draw a random latent vector from a unit Gaussian\n",
        "latent_vector = np.random.normal(size=(1, 200))\n",
        "\n",
        "# Generate a new image from the latent vector\n",
        "generated_image = decoder.predict(latent_vector)\n",
        "\n",
        "# Plot the generated image\n",
        "plt.imshow(generated_image[0])\n",
        "plt.axis('off')\n",
        "plt.title('Generated Image')\n",
        "plt.show()\n",
        "\n",
        "#You can repeat this process multiple times to generate a set of new images, and plot them together to visualize the generative capabilities of the VAE model.\n",
        "# Draw a set of latent vectors from a unit Gaussian\n",
        "latent_vectors = np.random.normal(size=(10, 200))\n",
        "\n",
        "# Generate a set of new images from the latent vectors\n",
        "generated_images = decoder.predict(latent_vectors)\n",
        "\n",
        "# Plot the generated images\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(10):\n",
        "    plt.subplot(5, 5, i+1)\n",
        "    plt.imshow(generated_images[i])\n",
        "    plt.axis('off')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}